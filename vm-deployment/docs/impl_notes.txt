That log is good news in one sense: you’re past the original vLLM “not enough free GPU memory on startup” failure. The engine loads, you start adding requests, and then you crash during the first actual execution.

This is not “missing python bindings”. A segfault here almost always means a native extension / JIT kernel path blew up (Torch / Triton / vLLM custom ops).

The most likely culprit: vLLM LoRA on V100 (Punica)

Your earlier stack trace showed the crash inside:

vllm/lora/.../triton_ops/lora_expand_op.py

vLLM’s LoRA path uses Punica kernels, and upstream has explicitly documented limitations around older NVIDIA compute capabilities (and users report even “single LoRA” can still go through Punica).

A Tesla V100 is sm70, i.e. < 8.0, so this matches perfectly.

Fast verification (do this first)

Run the exact same job but disable LoRA for vLLM:

In config: temporarily remove cot_adapter_path / vllm.adapter_path (or set to null)

In load_vllm_engine: set enable_lora=False and return lora_request=None

If the segfault disappears, you’ve confirmed it’s the LoRA(Punica/Triton) path.

Robust fix: auto-disable vLLM LoRA on < sm80

Add a hard guard in load_vllm_engine so you don’t accidentally hit this on V100 again:

# at top of load_vllm_engine
import torch

cap = (0, 0)
if torch.cuda.is_available():
    cap = torch.cuda.get_device_capability()
supports_vllm_lora = cap >= (8, 0)

# when building engine_kwargs:
engine_kwargs = dict(
    model=model_path,
    dtype=dtype,
    tensor_parallel_size=tp,
    gpu_memory_utilization=gmu,
    enable_lora=bool(adapter_path) and supports_vllm_lora,
    max_lora_rank=max_lora_rank,
)

if adapter_path and not supports_vllm_lora:
    logger.warning(
        "[vLLM] LoRA requested but GPU compute capability is %s; "
        "vLLM LoRA (Punica) requires >= 8.0. Disabling LoRA to avoid crash.",
        cap,
    )
    adapter_path = None
    lora_request = None


…and only create LoRARequest if supports_vllm_lora is true.

If you need the adapter: merge LoRA into the base model (no runtime LoRA)

This is the portable/self-contained workaround vLLM folks recommend when you only have one adapter: merge it and run the merged model as a normal model.

High level:

Run a one-off job that loads base + adapter with peft, calls merge_and_unload(), saves to e.g. /mnt/models/phi-4-merged-cot.

Point vLLM model_path to the merged directory.

Run vLLM with enable_lora=False forever after.

This avoids Punica entirely.

Two extra “stability” knobs (optional, but useful while testing)

Turn on vLLM debug logs: VLLM_LOGGING_LEVEL=DEBUG

Force eager mode to avoid CUDA-graph/compile paths while debugging: pass enforce_eager=True to LLM(...) (it disables CUDA graphs).

Also do this once: clear Triton caches

Since you previously failed during Triton compilation (no compiler), you can end up with junk in cache directories. Before rerunning:

wipe Triton + extensions cache (wherever they are pointing), or

set fresh dirs per job:

TRITON_CACHE_DIR=/mnt/cache/triton

TORCH_EXTENSIONS_DIR=/mnt/cache/torch_extensions