
Job Scripts on HPC (Colossus)
Table of Contents

    A Simple Serial Job
    Work Directory
    Splitting a Job into Tasks (Array Jobs)
    Parallel Jobs
        OpenMP or Threading
        MPI
        Combining MPI with OpenMP or threading
    Large Memory Jobs
        Accounting of Large Memory Jobs
    GPU jobs
    Checkpointing
    Useful sbatch parameters

  
This page documents how to write job scripts to submit jobs on the Colossus (HPC for TSD) cluster.

To run a job on the cluster involves creating a shell script called a job script. The job script is a plain-text file containing any number of commands, including your main computational task, i.e., it may copy or rename files, cd into the proper directory, etc., all before doing the "real" job. The lines in the script file are the commands to be executed, in the given order. Lines starting with a "#" are ignored as comments, except lines that start with a "#SBATCH" which are not executed, but contain special instructions to the queue system.

If you are not familiar with shell scripts, they are simply a set of commands that you could have typed at the command line. You can find more information about shell scripts here: Introduction to Bash shell scripts.

A job script consists of a couple of parts:

    Instructions to the queue system
    Commands to set up the execution environment
    The actual commands you want to be run

Instruction parameters to the queue system may be specified on the sbatch command line and/or in #SBATCH lines in the job script. There can be as many #SBATCH lines as you need, and you can combine several parameters on the same line. If a parameter is specified both on the command line and in the jobscript, the parameter specified on the command line takes precedence. The #SBATCH lines should precede any commands in the script. A couple of parameters are compulsory. If they are not present, the job will not run:

    --account: Specifies the project the job will run in. If your project has a Sigma quota, this will be project (e.g. p11). If your project does not have a Sigma quota you'll need to use the tsd reservation, project_tsd (e.g. p11_tsd).
    --time: Specifies the maximal wall clock time of the job. Do not specify the limit too low, because the job will be killed if it has not finished within this time. On the other hand, shorter jobs will be started sooner, so do not specify longer than you need. The default maximum allowed --time specification is 1 week; see details here.
    --mem-per-cpu: Specifies how much RAM each task (default is 1 task; see Parallel Jobs) of the job needs. Technically, this limit specifies the amount of resident memory + swap the job can use. If the job tries to use more than this setting, it will be killed. The maximum that can be requested it the available memory on the node (487 GiB on the normal compute nodes). For instance:

    #SBATCH --mem-per-cpu=2000M

    If the job tries to use more than 2000 MiB resident memory requested in this example, it will be killed. Note that --mem-per-cpu is specified per requested core.

The commands to set up the environment should include

module load SomeProgram/SomeVersion

jobsetup will set up needed environment variables and shell functions, and must be the first command in the script. module will set up environment variables to get access to the specified program.  It is recommended to specify the explicit version in the module load command.  We also encourage you to use

module purge

prior to the module load statements, to avoid inheriting unknown environment variable settings from the shell you use to submit the job.  We also advice using

set -o errexit

early in the script.  This makes the script exit immediately if any command below it fails, instead of simply going on to the next command.  This makes it much easier to find out whether anything went wrong in the job, and if so, where it happened.



A Simple Serial Job

This is a script for running a simple, serial job

#!/bin/bash

# Job name:
#SBATCH --job-name=YourJobname
#
# Project:
#SBATCH --account=YourProject
#
# Wall clock limit:
#SBATCH --time=hh:mm:ss
#
# Max memory usage:
#SBATCH --mem-per-cpu=Size

## Set up job environment:
module purge   # clear any inherited modules
set -o errexit # exit on errors

## Copy input files to the work directory:
cp MyInputFile $SCRATCH

## Make sure the results are copied back to the submit directory (see Work Directory below):
chkfile MyResultFile

## Do some work:
cd $SCRATCH
YourCommands

Substitute real values for YourJobname, YourProject, hh:mm:ss, Size (with 'M' for megabytes or 'G' for gigabytes; e.g. 200M or 1G), MyInputFile, MyResultFile and YourCommands.

Work Directory

Each job has access to a separate scratch space directory on the shared file system /cluster. The name of the directory is stored in the environment variable $SCRATCH in the job script. As a general rule, all jobs should use the scratch directory ($SCRATCH) as its work directory. This is especially important if the job uses a lot of files, or does much random access on the files (repeatedly read and write to different places of the files).

There are several reasons for using $SCRATCH:

    $SCRATCH is on a faster file system than user home directories.
    There is less risk of interfering with running jobs by accidentally modifying or deleting the jobs' input or output files.
    Temporary files are automatically cleaned up, because the scratch directory is removed when the job finishes.
    It avoids taking unneeded backups of temporary and partial files, because $SCRATCH is not backed up.

The directory where you ran sbatch is stored in the environment variable $SLURM_SUBMIT_DIR. If you want automatic copying of files or directories back to $SLURM_SUBMIT_DIR when the job is terminated, mark them with the command chkfile in the job script:

chkfile OneFile AnotherFile SomeDirectory

    The chkfile command should be placed early in the script, before the main computational commands: The files will be copied back even if the script crashes (even when you use set -o errexit), but not if it is terminated before it got to the chkfile command.
    If you want to use shell-metacharacters, they should be quoted.  I.e., use chkfile "Results*" instead of chkfile Results*.
    Do not use absolute paths or things like ".*" or .. in the chkfile command; use paths relative to $SCRATCH.
    The files/directories are copied to $SLURM_SUBMIT_DIR with rsync, which means that if you use chkfile ResultDir, the contents of ResultDir will be copied into $SLURM_SUBMIT_DIR/ResultDir, but if you use chkfile ResultDir/ (i.e., with a trailing /), the contents of ResultDir will be copied directly into $SLURM_SUBMIT_DIR.

We recommend using chkfile (or cleanup; see below) instead of explicitly copying the result files with cp.

For instance:

#!/bin/bash
#SBATCH --job-name=YourJobname --account=YourProject
#SBATCH --time=hh:mm:ss --mem-per-cpu=Size

module purge   # clear any inherited modules
set -o errexit # exit on errors

## Copy files to work directory:
cp YourDatafile $SCRATCH

## Mark outfiles for automatic copying to $SLURM_SUBMIT_DIR:
chkfile YourOutputfile

## Run command
cd $SCRATCH
YourProgram YourDatafile > YourOutputfile

The $SCRATCH directory is removed upon job exit (after copying back chkfiled files).

If you want more flexibility than what chkfile gives, you can use the command cleanup instead. It is used to specify commands to run when your job exits (before the $SCRATCH directory is removed). Just like chkfile, the cleanup commands are run even if your script crashes (even when you use set -o errexit), but not if it crashes before reaching the cleanup command, so place the command early in the script.

For instance:

cleanup "cp $SCRATCH/outputfile /some/other/directory/newName"

Note: do not use single quotes (') if the commands contain variables like $SCRATCH and $SLURM_SUBMIT_DIR.


Splitting a Job into Tasks (Array Jobs)

To run many instances of the same job, use the --array switch to sbatch. This is useful if you have a lot of data-sets which you want to process in the same way with the same job-script:

sbatch --array=from-to [other sbatch switches] YourScript

You can also put the --array switch in an #SBATCH line inside the script. from and to are the first and last task number. Each instance of YourScript can use the environment variable $SLURM_ARRAY_TASK_ID for selecting which data set to use, etc. For instance:

sbatch --array=1-100 MyScript

will run 100 copies of MyScript, setting the environment variable $SLURM_ARRAY_TASK_ID to 1, 2, ..., 100 in turn.

It is possible to specify the task ids in other ways than from-to: it can be a single number, a range (from-to), a range with a step size (from-to:step), or a comma separated list of these. Finally, adding %max at the end of the specification puts a limit on how many tasks will be allowed to run at the same time. A couple of examples:

Specification   Resulting TASK_IDs
1,4,42        # 1, 4, 42
1-5           # 1, 2, 3, 4, 5
0-10:2        # 0, 2, 4, 6, 8, 10
32,56,100-200 # 32, 56, 100, 101, 102, ..., 200
1-200%10      # 1, 2, ..., 200, but maximum 10 running at the same time

Note: spaces, decimal numbers or negative numbers are not allowed.

The instances of an array job are independent, they have their own $SCRATCH and are treated like separate jobs. (This means that they count against the limit of the total number of jobs a user can have in the job queue, currently 400.) You may also specify a parallel environment in array-jobs, so that each instance gets e.g. 8 processors. To cancel all tasks of an array job, cancel the jobid that is returned by the sbatch command.

See man sbatch for details.

An extended example:

$ cat JobScript
#!/bin/bash
#SBATCH --account=YourProject
#SBATCH --time=1:0:0
#SBATCH --mem-per-cpu=1G
#SBATCH --array=1-200

module purge   # clear any inherited modules
set -o errexit # exit on errors

DATASET=dataset.$SLURM_ARRAY_TASK_ID
OUTFILE=result.$SLURM_ARRAY_TASK_ID

cp $DATASET $SCRATCH
cd $SCRATCH
chkfile $OUTFILE
YourProgram $DATASET > $OUTFILE

$ sbatch JobScript

This job will process the datasets dataset.1, dataset.2, ... dataset.200 and leave the results in result.1, result.2, ... result.200.


Parallel Jobs

Since the clusters consist of multi-cpu and multi-core compute nodes, there are several ways of parallelising code and running jobs in parallel. One can use MPI, OpenMP and threading. (For SIMD jobs and other jobs with very independent parallel tasks, array jobs is a good alternative.) In this section we will explain how to run in parallel in either of these ways.

Note: A parallel job will get one, shared scratch directory ($SCRATCH), not separate directories for each node! This means that if more than one process or thread write output to disk, they must use different file names, or put the files in different subdirectories.
OpenMP or Threading

In order to run in parallel on one node using either threading or OpenMP, the only thing you need to remember is to tell the queue system that your task needs more than one core, all on the same node. This is done with the --cpus-per-task switch.  It will reserve the needed cores on the node and set the environment variable $OMP_NUM_THREADS. For example:

#!/bin/bash
# Job name:
#SBATCH --job-name=YourJobname
#
# Project:
#SBATCH --account=YourProject
#
# Wall clock limit:
#SBATCH --time=hh:mm:ss
#
# Max memory usage per core (MB):
#SBATCH --mem-per-cpu=MegaBytes
#
# Number of cores:
#SBATCH --cpus-per-task=NumCores

## Set up job environment:
module purge   # clear any inherited modules
set -o errexit # exit on errors

## Copy files to work directory:
cp YourDatafile $SCRATCH

## Mark outfiles for automatic copying to $SLURM_SUBMIT_DIR:
chkfile YourOutputfile

## Run command
cd $SCRATCH
## (For non-OpenMP-programs, you must control the number of threads manually, using $OMP_NUM_THREADS.)
YourCommand YourDatafile > YourOutputfile

The --cpus-per-task will ensure all cores are allocated on a single node. If you ask for more cores than is available on any node, you will not be able to submiut the job. Most nodes on Colossus have 64 cores.
MPI

To run MPI jobs, you must specify how many tasks to run (i.e., cores to use), and set up the desired MPI environment. The symbol generation for different fortran compilers differ, hence versions of the MPI fortran interface for GNU, Intel, Portland and Open64. All OpenMPI libraries are built using gcc.

You need to use module load both in order to compile your code and to run it. You also should compile and run in the same MPI-environment. Although some MPI-versions may be compatible, they usually are not.

If you need to compile C MPI code with icc please see OpenMPI's documentation about environment variables to set in order to force mpicc to use icc.

The simplest way to specify the number of tasks (cores) to use in an MPI job, is to use the sbatch switch --ntasks. For instance

#SBATCH --ntasks 10

would give you 10 tasks. The queue system allocates the tasks to nodes depending on available cores and memory, etc. A simple MPI jobscript can then be like:

#!/bin/bash
# Job name:
#SBATCH --job-name=YourJobname
#
# Project:
#SBATCH --account=YourProject
#
# Wall clock limit:
#SBATCH --time=hh:mm:ss
#
# Max memory usage per task:
#SBATCH --mem-per-cpu=Size
#
# Number of tasks (cores):
#SBATCH --ntasks=NumTasks

## Set up job environment:
module purge   # clear any inherited modules
set -o errexit # exit on errors

module load openmpi.gnu

## Set up input and output files:
cp InputFile $SCRATCH
chkfile OutputFile

cd $SCRATCH
srun YourCommand

Queue System Options

Using the queue option --ntasks in the previous example, we have assumed that it doesn't matter how your tasks are allocated to nodes. The queue system will run your tasks on nodes as it sees fit (however, it will try to allocate as many tasks as possible on each node). For small jobs, that is usually OK. Sometimes, however, you might need more control. Then you can use the switches --nodes and --ntasks-per-node instead of --ntasks:

    --nodes: How many nodes to use
    --ntasks-per-node: How many tasks to run on each node.

For instance, to get 1 task on each of 4 nodes, you can use

#SBATCH --nodes=4 --ntasks-per-node=1

Or, to use 4 task on each of 2 nodes:

#SBATCH --nodes=2 --ntasks-per-node=4

There are more advanced options for selecting cores and nodes, as well. See man sbatch for the gory details.
TCP/IP over InfiniBand for MPI

If you have MPI jobs hard linked to use TCP/IP we have some tricks to use InfiniBand even for these. It is possible to run the TCP/IP over InfiniBand with far better performance than over Ethernet. However this only apply to communications between the compute nodes. Please contact us if you have such an application or want to use TCP/IP over InfiniBand. All nodes have two IP numbers one for the Ethernetnet and one for InfiniBand.
Useful Commands in MPI Scripts

If you need to execute a command once on each node of a job, you can use

srun --ntasks=$SLURM_JOB_NUM_NODES command

Combining MPI with OpenMP or threading

You can combine MPI with OpenMP or threading, such that MPI is used for launching multi-threaded processes on each node. The best way to do this is:

#!/bin/bash
# Job name:
#SBATCH --job-name=YourJobname
#
# Project:
#SBATCH --account=YourProject
#
# Wall clock limit:
#SBATCH --time=hh:mm:ss
#
# Max memory usage per task:
#SBATCH --mem-per-cpu=Size
#
# Number of tasks (MPI ranks):
#SBATCH --ntasks=NumTasks
#
# Number of threads per task:
#SBATCH --cpus-per-task=NumThreads

## Set up job environment:
module purge   # clear any inherited modules
set -o errexit # exit on errors
module load openmpi.gnu

## Set up input/output files:
cp InputFile $SCRATCH
chkfile OutputFile

## Run command
## (If YourCommand is not OpenMP, use $OMP_NUM_THREADS to control the number of threads manually.)
srun --cpus-per-task=$SLURM_CPUS_PER_TASK YourCommand

This makes srun start NumTasks ranks (processes), each of which having NumThreads threads. If you are not using OpenMP, your program must make sure not to start more than NumThreads threads.

Just as with single-threaded MPI jobs, you can get more than one rank (MPI process) on each node. If you need more control over how many ranks are started on each node, use --ntasks-per-node and --nodes as above. For instance:

#SBATCH --nodes=3 --ntasks-per-node=2 --cpus-per-task=4

will start 2 MPI ranks on each of 3 machines, and each process is allowed to use 4 threads.


Large Memory Jobs

Most nodes on Colossus are equipped with 64 CPU cores and 487 GiB of RAM usable for jobs. There are also a couple of special bigmem nodes with 128 cores and 4 TiB memory, of which about 3955 GiB can be used for jobs.

If you need more than 487 GiB RAM on a single node, you must specify --partition=bigmem to get access to the nodes with more RAM. For instance

#SBATCH --ntasks-per-node=16
#SBATCH --mem-per-cpu=50G --partition=bigmem

Accounting of Large Memory Jobs

To ensure maximal utilisation of the cluster, memory usage is accounted as well as cpu usage. Memory specifications are converted to "Processor Equivalents" (PE) using a conversion factor of approximately 8 GiB / core(*). If a job specifies more than 8 GiB RAM per task, i.e., --mem-per-cpu=M, where M > 8G, each task will count as M / 8G cores instead of 1 core. For instance, a job with --ntasks=2 --mem-per-cpu=16G will be counted as using 4 cores instead of 2.

The reason for this is that large memory jobs make the "unused" cores on the same nodes inaccessible to other jobs. For instance, a job on a 487 GiB node using --ntasks=1 --mem-per-cpu=487G will in practice use all cores on the node, and should be accounted as such.

Note that only jobs which specify more than 8 GiB per core will be affected by this; all other jobs will be accounted with the number of tasks specified.

(*) The exact value of the factor depends on the total amount of RAM per core in the cluster, and is currently about 8 GiB / core on Colossus.


GPU jobs

Colossus has GPU nodes with 2 (Nvidia Tesla V100; TSD allocation) or 4 (Nvidia Tesla A100; Sigma2 allocation) GPUs each. You can run jobs on the GPU nodes like this in the TSD queue, where N is the number of GPUs:

 $ sbatch --account=YourProject_tsd --partition=accel --gres=gpu:N

or like this in the Sigma2 queue:

 $ sbatch --account=YourProject --partition=accel --gres=gpu:N


 Checkpointing

Checkpointing a job means that the job can be stopped and started somewhere else, and continues where it left off.

Long-running jobs should implement some form of checkpointing, by saving intermediate results at intervals and being able to start with the latest intermediate results if restarted.


Useful sbatch parameters
Parameter 	Description
--account=project 	Specify the project to run under. This parameter is required.
--begin=time 	Start the job at a given time
--constraint=feature 	Request nodes with a certain feature. Currently supported features include amdm intel, ib, rackN. If you need more than one feature, they must be combined with & in the same --constraint specification, e.g. --constraint=ib&rack21. Note: If you try to use more than one --constraint specification, the last one will override the earlier.
--cpus-per-task=cores 	Specify the number of cpus (actually: cores) to allocate for each task in the job.  See --ntasks and --ntasks-per-node, or man sbatch.
--dependency=dependency list 	Defer the start of this job until the specified dependencies have been satisfied. See man sbatch for details.
--error=file 	Send 'stderr' to the specified file. Default is to send it to the same file as 'stdout'. (Note: $HOME or ~ cannot be used. Use absolute or relative paths instead.)
--input=file 	Read 'stdin' from the specified file. (Note: $HOME or ~ cannot be used. Use absolute or relative paths instead.)
--job-name=jobname 	Specify job name
--mem-per-cpu=size 	Specify the memory required per allocated core. This is the normal way of specifying memory requirements (see Large Memory Jobs). size should be an integer followed by 'M' or 'G'.
--partition=hugemem 	Run on a hugemem node (see Large Memory Jobs).
--nodes=nodes 	Specify the number of nodes to allocate. nodes can be an integer or a range (min-max). This is often combined with --ntasks-per-node.
--ntasks=tasks 	Specify the number of tasks (usually cores, but see --cpus-per-task) to allocate. This is the usual way to specify cores in MPI jobs.
--ntasks-per-node=tasks 	Specify the number of tasks (usually cores, but see --cpus-per-task) to allocate within each allocated node. Often combined with --nodes.
--output=file

Send 'stdout' (and 'stderr' if not redirected with --error) to the specified file instead of slurm-%j.out.

Note:

    $HOME or ~ cannot be used. Use absolute or relative paths instead.
    The jobid should always be included in the filename in order to produce a separate file for each job (i.e. --output=custom-name%j.out). This is necessary for error tracing.

--time=time 	Specify a (wall clock) time limit for the job. time can be hh:mm:ss or dd-hh:mm:ss. This parameter is required.
--nice=value 	Run the job with an adjusted scheduling priority within your project. A negative nice value increases the priority, otherwise decreases it. The adjustment range is +/- 2147483645.



Containers of VM (TSD)
Table of Contents

    Introduction
        Support level
        Available container platforms
    Podman
        Usage
        Importing container images
        Container building
        Container limitations
    Singularity
        Singularity usage
        Singularity limitations

Introduction

We offer support for projects to run Linux containers in TSD if requested. Note that this is not a part of the base TSD project configuration, and so project administrators should contact us to request installation.

To facilitate container use, we deploy a special-purpose virtual machine running a container engine that allows project members to run OCI/Docker container images that they import into TSD.
Support level

TSD provides the container platform and the maintenance of it, while projects provide their own container images and are responsible for operating them.
Available container platforms

In TSD we offer Podman and Singularity for running containers.

TSD provides a deployment of the Podman container engine configured to be safe for project use. The container engine will be running as a non-root user and containers will be run in Linux user namespaces, but for the most part this should not differ much from running containers elsewhere in common usage.

The Podman container engine will be installed on a special-purpose containers host that project members can connect to via SSH from the project’s login node.

Unlike Podman, Singularity can be installed on regular project machines if requested. It is also available as a Colossus module for use in HPC jobs. We use the packages provided by the Extra Packages for Enterprise Linux (“EPEL”) project.
Podman
Usage

Podman offers completely a docker compatible command-line interface, as well as a Docker API v1.40 compatible API implementation for controlling the container engine.

Either docker or podman can be used interchangably within TSD when interacting with the container service.

As Internet registries aren’t available on the inside, container images need to be imported manually into TSD, which is documented below. Otherwise, the podman/docker command-line interface can be used in the same way as you normally would.
Examples

Example of running an Alpine 3.14 container interactively with pXXXX/data/no-backup bind mounted to /no-backup:

$ docker run --rm -ti -v /tsd/p1337/data/no-backup:/no-backup docker.io/alpine:3.14
/ # cat /etc/os-release
NAME="Alpine Linux"
ID=alpine
VERSION_ID=3.14.2
PRETTY_NAME="Alpine Linux v3.14"
HOME_URL="https://alpinelinux.org/"
BUG_REPORT_URL="https://bugs.alpinelinux.org/"
/ # id
uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video)
/ # uname -a > /no-backup/alpine
/ # cat /no-backup/alpine
Linux 946afc425f65 4.18.0-305.19.1.el8_4.x86_64 #1 SMP Tue Sep 7 07:07:31 EDT 2021 x86_64 Linux
/ # ls -l /no-backup/
total 96
-rw-r--r--    1 root     bin             96 Nov  8 14:17 alpine
-rw-r--r--    1 root     bin             14 Nov  2 14:25 newfiles
-rw-r--r--    1 root     bin             22 Oct 29 08:20 test

Example of running an nginx container with host TCP port 8080 redirected to container port 80:

$ docker run -d -p 8080:80 docker.io/library/nginx:1.20.1-alpine
e501d9d4e4253dc9def589d2ec8147aaa661b37b96c58c99872883ed1892b9c8
$ curl http://127.0.0.1:8080
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

Importing container images

On the outside of TSD, pull and save a container image you wish to bring into TSD. As an example, we will show how this would be done in the case of nginx version 1.21.3 from Docker Hub:

$ docker pull docker.io/nginx:1.21.3
Trying to pull docker.io/library/nginx:1.21.3...
Getting image source signatures
Copying blob b380bbd43752 skipped: already exists
Copying blob fca7e12d1754 done
Copying blob 1c84ebdff681 done
Copying blob a4723e260b6f done
Copying blob 745ab57616cb done
Copying blob 858292fd2e56 done
Copying config 87a94228f1 done
Writing manifest to image destination
Storing signatures
87a94228f133e2da99cb16d653cd1373c5b4e8689956386c1c12b60a20421a02

$ docker save docker.io/nginx:1.21.3 -o nginx-1.21.3.tar
Getting image source signatures
Copying blob e81bff2725db done
Copying blob 43f4e41372e4 done
Copying blob 788e89a4d186 done
Copying blob f8e880dfc4ef done
Copying blob f7e00b807643 done
Copying blob 9959a332cf6e done
Copying config 87a94228f1 done
Writing manifest to image destination
Storing signatures

You can then go to the TSD Data Portal and upload the prepared tarball into your project’s storage area.

On the inside of TSD, podman load/docker load the container tarball on the project’s container host. In the case of the above prepared nginx image:

$ docker load -i /tsd/pXXXX/data/durable/file-import/pXXXX-member-group/nginx-1.21.3.tar
Getting image source signatures
Copying blob 9959a332cf6e done
Copying blob 43f4e41372e4 done
Copying blob e81bff2725db skipped: already exists
Copying blob 788e89a4d186 done
Copying blob f7e00b807643 done
Copying blob f8e880dfc4ef done
Copying config 87a94228f1 done
Writing manifest to image destination
Storing signatures
Loaded image(s): docker.io/library/nginx:1.21.3

Container building

It’s possible to build container images within TSD, but as you are practically operating in an offline environment, you will not be able to pull arbitrary packages from Internet resources.

We recommend that you build your container images outside of TSD, and import the finalized images using the above instructions.
Container limitations

There are some limitations when running “rootless” containers, but for TSD users the most important caveats to note are:

    Ports below 1024 can’t be bound on the host system (within the containers you can bind any port you want).
    Users in container namespaces do not inherit the group memberships of the user running them. Normally containers are configured to only receive membership in the pXXXX-member-group.
    Network shares can only be bind mounted into a container from the top level (for instance pXXXX/data/durable can be mounted, but a directory further down in the hierarchy can not be directly mounted).

Singularity

The most widely used container system for HPC.
Singularity usage

Can be used in TSD as elsewhere, please refer to the upstream documentation.
Singularity limitations

The Singularity modules available in the Colossus module system can not be used on project machines, only in HPC jobs.

If you wish to use Singularity on TSD project hosts, please contact us and let us know which hosts you want it installed on.


Containers on HPC (Colossus):
Table of Contents

    Description
    Home page
    Documentation
    License
    Usage
        Use on submit nodes
        Use on Colossus
        Binding filesystems
        Building Apptainer images
        GPU containers

Description

Apptainer (formerly singularity) enables users to have full control of their environment. This means that a non-privileged user can "swap out" the operating system on the host for one they control. So if the host system is running RHEL9 but your application runs in Ubuntu, you can create an Ubuntu image, install your applications into that image, copy the image to another host, and run your application on that host in it's native Ubuntu environment!
Home page

https://apptainer.org/
Documentation

https://apptainer.org/documentation/
License

https://apptainer.org/docs/user/latest/license.html#license
Usage
Use on submit nodes
Apptainer is installed on the pXX-hpc-01 submit hosts as a system package. If it's not installed, please contact TSD support.
Use on Colossus

Apptainer is installed on the compute nodes as a system package (not a software module).

To run an Apptainer container in a job script, use

apptainer exec [exec options...] <container-path> <command>

Use

apptainer --help

to get help.
Binding filesystems

On Colossus the GPFS filesystem is mounted under /gpfs with symlinks from /cluster to mirror the setup on the submit hosts. By default, Apptainer will bind $HOME, /gpfs and /cluster into Apptainer containers that run on the compute nodes (what you submit via a job script) to allow reading from and writing to durable, /cluster and $HOME.

To specify additional bind paths use the --bind argument:

apptainer exec --bind <path1>,<path2> <container-path> <command>

Building Apptainer images

Due to a lack of admin privileges and internet access, TSD does not support building Apptainer images inside TSD. Apptainer images must be built outside TSD and then imported into TSD. Please reference the Apptainer documentation for local installation on Linux, Mac or Windows and how to build containers.
Building containers from Apptainer definition files (outside TSD)

    Create a bootstrap definition file:

Bootstrap: docker
From: ubuntu:16.04

%post
    apt-get -y update
    apt-get -y install fortune cowsay lolcat

%environment
    export LC_ALL=C
    export PATH=/usr/games:$PATH

%runscript
    fortune | cowsay | lolcat

    Then build the image using the following command:

sudo apptainer build lolcow.sif lolcow.def

Converted Docker to apptainer containers (outside TSD)

Apptainer can directly fetch and build containers using Docker/OCI container images from a registry.

Example building a Apptainer Image Format container from Docker Hub's alpine:3 image:

apptainer build alpine.sif docker://docker.io/alpine:3

Please reference the documentation on Apptainer's interoperability with Docker.
GPU containers

To run GPU enabled containers on the GPU nodes, pass the --nv argument:

apptainer exec --nv <container-path> <command>




GPU/NVIDIA/CUDA
Hardware

Dual Intel Xeon(R) CPU E5-2609 @ 2.40GHz, aka Sandy Bridge with 64 GiB memory nodes each with two NVIDIA Tesla K20 (Kepler architecture) cards.

Each card comprise :

6 GiB device memory

2688 single precision and 896 dual precision cores

with a measured performance of more than 1 Tflops/s for matrix multiplication using single precision data type. If you want more information load the pgi/14.4 module and issue the command : pgaccelinfo.


Sofware

NVIDIA kernel driver

NVIDIA Cuda 5.0, 5.5, 6.0, 6.5, 7.0, 7.5 and 8.0 (the current default) are installed, module name cuda/X.Y , try module avail to see all choices.

The software is installed at $CUDA, try ls $CUDA

Here you'll find source code, documentation etc.

Portland Accelerated C/C++/Fortran compiler have support for CUDA.


Access

The GPU nodes are located in rack 19, named compute-19-1 through 16, og just c19-1 through 16. To gain access use qlogin with GPU request.

qlogin --account=<your account>  --partition=accel --gres=gpu:2

Use gpu:1 for one GPU and gpu:2 for both. You can also add --exclusive to reserve the complete node for your work.


Selecting GPU

There are two GPUs in each node. It is important to keep control of which one you are using if you have not reserved the node exclusively or if you run two GPU jobs simultaneously.

Use the environment variable CUDA_VISIBLE_DEVICES, set it to 0 or 1 depending on which one you have been allocated or which one you want to use if you have allocated both. SLURM set this variable for you.

Syntax export CUDA_VISIBLE_DEVICES=1 or 0, normally there is no need to change this variable.

The queue system will set it to 0 or 1 if you have only requested one, and "0,1" if you have requested both. You have to set variables accordingly in subshells to yse both simultaneously.

You can set CUDA_VISIBLE_DEVICES to 0 in one shell and 1 in another and run programs, or you can keep the 0,1 and use ACC_DEVICE_NUM to select the GPU when using Portland.



Accelerated Compiler

Log in to one of the GPU nodes with qlogin or ssh as it is only possible to compile and build GPU enabled programs on the GPU nodes.

Use the Portland compiler, load the module pgi/14.x and you're ready to use the accelerated compilers.

If you want to compile and run the NVIDIA example programs you'll need to load the cuda/6.x module, this is not necessary if you only work with Portland.



It's very simple to get started, Portland has a goal that accelerated computing should be as simple as OpenMP. Just insert directives in the source code and compile. It is a good idea to set the stack size to unlimited using the command ulimit -s unlimited



A piece of Fortran code might look like this:

!$acc region

do i = 1,n

  r(i) = a(i) * 2.0

enddo

!$acc end region

and compile with accelerator options:



pgfortran -ta=nvidia,kepler,time -Minfo=acc



Then it's just to launch it as any other program ./myprog.x and enjoy the performance provided by the accelerator. The more work you can move to the GPU the better, performance is generally limited by bandwidth to and from the GPU device.

There are two GPUs in each node. It is important to keep control of which one you are using if you have not reserved the node exclusively or if you run two GPU jobs simultaneously.

Use the environment variable ACC_DEVICE_NUM, set it to 0 or 1 depending on which one you have been allocated or which one you want to use if you have allocated both.

Syntax export ACC_DEVICE_NUM=1 or 0.




CUDA Libraries

NVIDIA provides a range of highly optimized functions that can be called from fortran or c code. The fortran functions are available through a wrapper which hides all the boring details. The wrapper routines are already compiled for Colossus and located together with the CUDA 64 bit libraries. The CUDA documentation is available in both pdf and html format. It's located with the CUDA software under doc. Look under $CUDA/doc/.



To use the CUDA BLAS libraries are remarkably simple,



In your fortran code replace the call to sgemm with the following line:



call cublas_sgemm('n', 'n', N, N, N, alpha, a, N, b, N, beta, c, N)



The parameter list is identical, the only change is to prefix the sgemm name with cublas_. The same is true for double, complex and double complex data types (s,d,c and z).



To compile there is a few more libraries to include, include the following on the link line:



-L/usr/local/cuda/lib64 /usr/local/cuda/lib64/fortran_thunking.o -lcublas

We have compiled the wrappers, but the source code can be found with the cuda library, $CUDA/src if you want to compile yourself.

An example may look like:

gfortran -o dgemmdriver.x -L/usr/local/cuda/lib64 /usr/local/cuda/lib64/fortran_thunking.o -lcublas dgemmdriver.f90



Quite remarkable performe can be obtained using the library in functions,



[olews@compute-19-4 Fortran-BLAS]$ ./sgemmdriver.x

Total footprint A,B & C 5538 MiB

CUDA sgemm start

CUDA sgemm end 15.0307150 secs 1416.83215 Gflops/s

CPU/MKL sgemm start

cpu time spent in threaded lib 146.434982 using 4 threads

 CPU/MKL dgemm end 146.434982 secs 145.429733 Gflops/s

Speedup 9.53373528

diff sum(c)_gpu sum(c)_cpu:0.0000000000E+00

[olews@compute-19-4 Fortran-BLAS]$

A speed 9 times the MKL using all cores in one socket could be obtained. The nodes have two GPU cards and two sockets in each node, each processor have only 4 cores on these GPU nodes and they also run a slightly lower clock frequency than the general compute nodes in Colossus. The performance using double precision (64 bit floats) is about one third of the performance using single precision, while the CPU/MKL performance is about one half when using double precision numbers compared to single precision. The GPU cards has 2688 single precision and 896 dual precision cores.



There are two GPUs in each node. It is important to keep control of which one you are using if you have not reserved the node exclusively or if you run two GPU jobs simultaneously.

Use the environment variable CUDA_VISIBLE_DEVICES, set it to 0 or 1 depending on which one you have been allocated or which one you want to use if you have allocated both.



Syntax export CUDA_VISIBLE_DEVICES=1 or 0.